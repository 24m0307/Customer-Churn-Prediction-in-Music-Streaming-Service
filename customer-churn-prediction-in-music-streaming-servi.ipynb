{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102660,"databundleVersionId":12368678,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:46:21.368377Z","iopub.execute_input":"2025-08-15T16:46:21.368653Z","iopub.status.idle":"2025-08-15T16:46:22.939888Z","shell.execute_reply.started":"2025-08-15T16:46:21.368628Z","shell.execute_reply":"2025-08-15T16:46:22.939121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass ChurnPredictionModel:\n    def __init__(self):\n        self.models = {}\n        self.feature_importance = {}\n        self.scaler = StandardScaler()\n        self.label_encoders = {}\n        \n    def load_data(self):\n        \"\"\"Load all datasets\"\"\"\n        print(\"Loading datasets...\")\n        \n        # Load all datasets\n        self.train_data = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/train_data.csv')\n        self.members = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/members.csv')\n        self.transactions = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/transactions.csv')\n        self.user_logs = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/user_logs.csv')\n        self.test_data = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/kaggle_test_data.csv')\n        \n        print(f\"Train data shape: {self.train_data.shape}\")\n        print(f\"Members data shape: {self.members.shape}\")\n        print(f\"Transactions data shape: {self.transactions.shape}\")\n        print(f\"User logs data shape: {self.user_logs.shape}\")\n        print(f\"Test data shape: {self.test_data.shape}\")\n        \n    def preprocess_data(self):\n        \"\"\"Clean and preprocess all datasets\"\"\"\n        print(\"Preprocessing data...\")\n        \n        # Clean members data\n        self.members['bd'] = self.members['bd'].apply(lambda x: x if 0 <= x <= 100 else np.nan)\n        self.members['registration_init_time'] = pd.to_datetime(self.members['registration_init_time'], format='%Y%m%d')\n        \n        # Clean transactions data\n        self.transactions['transaction_date'] = pd.to_datetime(self.transactions['transaction_date'], format='%Y%m%d')\n        self.transactions['membership_expire_date'] = pd.to_datetime(self.transactions['membership_expire_date'], format='%Y%m%d')\n        \n        # Clean user logs data\n        self.user_logs['date'] = pd.to_datetime(self.user_logs['date'], format='%Y%m%d')\n        \n    def engineer_features(self, data_type='train'):\n        \"\"\"Engineer features from all data sources\"\"\"\n        print(f\"Engineering features for {data_type} data...\")\n        \n        if data_type == 'train':\n            base_data = self.train_data.copy()\n        else:\n            base_data = self.test_data.copy()\n            \n        # Start with base data\n        features_df = base_data.copy()\n        \n        # 1. Member features\n        member_features = self.create_member_features()\n        features_df = features_df.merge(member_features, on='msno', how='left')\n        \n        # 2. Transaction features\n        transaction_features = self.create_transaction_features()\n        features_df = features_df.merge(transaction_features, on='msno', how='left')\n        \n        # 3. User behavior features\n        behavior_features = self.create_behavior_features()\n        features_df = features_df.merge(behavior_features, on='msno', how='left')\n        \n        return features_df\n    \n    def create_member_features(self):\n        \"\"\"Create features from members data\"\"\"\n        member_features = self.members.copy()\n        \n        # Age features\n        member_features['age'] = member_features['bd']\n        member_features['age_group'] = pd.cut(member_features['age'], \n                                            bins=[0, 18, 25, 35, 45, 55, 100], \n                                            labels=['<18', '18-25', '25-35', '35-45', '45-55', '55+'])\n        \n        # Registration features\n        member_features['registration_year'] = member_features['registration_init_time'].dt.year\n        member_features['registration_month'] = member_features['registration_init_time'].dt.month\n        member_features['days_since_registration'] = (pd.Timestamp('2017-03-31') - member_features['registration_init_time']).dt.days\n        \n        return member_features[['msno', 'city', 'age', 'age_group', 'gender', 'registered_via', \n                               'registration_year', 'registration_month', 'days_since_registration']]\n    \n    def create_transaction_features(self):\n        \"\"\"Create features from transaction data\"\"\"\n        # Get latest transaction for each user\n        latest_transactions = self.transactions.sort_values('transaction_date').groupby('msno').tail(1)\n        \n        # Aggregate transaction features\n        transaction_agg = self.transactions.groupby('msno').agg({\n            'payment_method_id': ['nunique', 'last'],\n            'payment_plan_days': ['mean', 'std', 'last'],\n            'plan_list_price': ['mean', 'std', 'last'],\n            'actual_amount_paid': ['mean', 'std', 'sum', 'last'],\n            'is_auto_renew': ['last', 'mean'],\n            'is_cancel': ['sum', 'mean', 'last'],\n            'transaction_date': ['count', 'last']\n        }).round(2)\n        \n        # Flatten column names\n        transaction_agg.columns = ['_'.join(col).strip() for col in transaction_agg.columns]\n        transaction_agg = transaction_agg.reset_index()\n        \n        # Add features from latest transaction\n        latest_features = latest_transactions[['msno', 'membership_expire_date']].copy()\n        latest_features['days_to_expire'] = (latest_features['membership_expire_date'] - pd.Timestamp('2017-03-31')).dt.days\n        \n        # Merge features\n        transaction_features = transaction_agg.merge(latest_features, on='msno', how='left')\n        \n        # Calculate discount rate\n        transaction_features['discount_rate'] = (\n            transaction_features['plan_list_price_last'] - transaction_features['actual_amount_paid_last']\n        ) / transaction_features['plan_list_price_last']\n        \n        return transaction_features\n    \n    def create_behavior_features(self):\n        \"\"\"Create features from user logs data\"\"\"\n        # Calculate behavior metrics for each user\n        behavior_agg = self.user_logs.groupby('msno').agg({\n            'num_25': ['sum', 'mean', 'std'],\n            'num_50': ['sum', 'mean', 'std'],\n            'num_75': ['sum', 'mean', 'std'],\n            'num_985': ['sum', 'mean', 'std'],\n            'num_100': ['sum', 'mean', 'std'],\n            'num_unq': ['sum', 'mean', 'std', 'max'],\n            'total_secs': ['sum', 'mean', 'std', 'max'],\n            'date': ['count']  # number of active days\n        }).round(2)\n        \n        # Flatten column names\n        behavior_agg.columns = ['_'.join(col).strip() for col in behavior_agg.columns]\n        behavior_agg = behavior_agg.reset_index()\n        \n        # Calculate derived features\n        total_songs = (behavior_agg['num_25_sum'] + behavior_agg['num_50_sum'] + \n                      behavior_agg['num_75_sum'] + behavior_agg['num_985_sum'] + \n                      behavior_agg['num_100_sum'])\n        \n        behavior_agg['total_songs'] = total_songs\n        behavior_agg['completion_rate'] = behavior_agg['num_100_sum'] / (total_songs + 1)\n        behavior_agg['skip_rate'] = behavior_agg['num_25_sum'] / (total_songs + 1)\n        behavior_agg['engagement_score'] = (behavior_agg['num_75_sum'] + behavior_agg['num_985_sum'] + \n                                           behavior_agg['num_100_sum']) / (total_songs + 1)\n        behavior_agg['avg_daily_songs'] = total_songs / (behavior_agg['date_count'] + 1)\n        behavior_agg['avg_daily_listening_time'] = behavior_agg['total_secs_sum'] / (behavior_agg['date_count'] + 1)\n        \n        return behavior_agg\n    \n    def prepare_model_data(self, features_df):\n        \"\"\"Prepare data for modeling\"\"\"\n        # Encode categorical variables\n        categorical_cols = ['city', 'age_group', 'gender', 'registered_via']\n        \n        for col in categorical_cols:\n            if col in features_df.columns:\n                if col not in self.label_encoders:\n                    self.label_encoders[col] = LabelEncoder()\n                    features_df[col] = self.label_encoders[col].fit_transform(features_df[col].astype(str))\n                else:\n                    features_df[col] = self.label_encoders[col].transform(features_df[col].astype(str))\n        \n        # Fill missing values\n        numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n        features_df[numeric_cols] = features_df[numeric_cols].fillna(features_df[numeric_cols].median())\n        \n        return features_df\n    \n    def train_models(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train multiple models\"\"\"\n        print(\"Training models...\")\n        \n        # Define models\n        models = {\n            'random_forest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n            'gradient_boosting': GradientBoostingClassifier(n_estimators=200, random_state=42),\n            'logistic_regression': LogisticRegression(random_state=42),\n            'lightgbm': lgb.LGBMClassifier(random_state=42, n_jobs=-1),\n            'xgboost': xgb.XGBClassifier(random_state=42, n_jobs=-1)\n        }\n        \n        results = {}\n        \n        for name, model in models.items():\n            print(f\"Training {name}...\")\n            \n            # Train model\n            model.fit(X_train, y_train)\n            \n            # Predict on validation set\n            y_pred = model.predict(X_val)\n            y_pred_proba = model.predict_proba(X_val)[:, 1]\n            \n            # Calculate metrics\n            auc_score = roc_auc_score(y_val, y_pred_proba)\n            \n            results[name] = {\n                'model': model,\n                'auc_score': auc_score,\n                'predictions': y_pred,\n                'probabilities': y_pred_proba\n            }\n            \n            print(f\"{name} AUC: {auc_score:.4f}\")\n            \n            # Store feature importance for tree-based models\n            if hasattr(model, 'feature_importances_'):\n                self.feature_importance[name] = model.feature_importances_\n        \n        self.models = results\n        return results\n    \n    def create_ensemble(self, X_val, y_val):\n        \"\"\"Create ensemble prediction\"\"\"\n        print(\"Creating ensemble...\")\n        \n        # Weight models by their AUC scores\n        total_auc = sum([result['auc_score'] for result in self.models.values()])\n        weights = {name: result['auc_score'] / total_auc for name, result in self.models.items()}\n        \n        # Create weighted ensemble\n        ensemble_proba = np.zeros(len(X_val))\n        for name, result in self.models.items():\n            ensemble_proba += weights[name] * result['probabilities']\n        \n        ensemble_auc = roc_auc_score(y_val, ensemble_proba)\n        print(f\"Ensemble AUC: {ensemble_auc:.4f}\")\n        \n        self.ensemble_weights = weights\n        return ensemble_proba\n    \n    def plot_feature_importance(self, model_name='lightgbm', top_n=20):\n        \"\"\"Plot feature importance\"\"\"\n        if model_name in self.feature_importance:\n            feature_names = self.feature_names\n            importance = self.feature_importance[model_name]\n            \n            # Get top features\n            feature_importance_df = pd.DataFrame({\n                'feature': feature_names,\n                'importance': importance\n            }).sort_values('importance', ascending=False).head(top_n)\n            \n            plt.figure(figsize=(10, 8))\n            sns.barplot(data=feature_importance_df, x='importance', y='feature')\n            plt.title(f'Top {top_n} Feature Importance - {model_name.upper()}')\n            plt.tight_layout()\n            plt.show()\n    \n    def generate_predictions(self, test_features):\n        \"\"\"Generate final predictions for test set\"\"\"\n        print(\"Generating final predictions...\")\n        \n        # Use ensemble approach\n        ensemble_proba = np.zeros(len(test_features))\n        \n        for name, weight in self.ensemble_weights.items():\n            model = self.models[name]['model']\n            proba = model.predict_proba(test_features)[:, 1]\n            ensemble_proba += weight * proba\n        \n        return ensemble_proba\n    \n    def run_complete_pipeline(self):\n        \"\"\"Run the complete modeling pipeline\"\"\"\n        print(\"=== CHURN PREDICTION MODEL PIPELINE ===\")\n        \n        # Load and preprocess data\n        self.load_data()\n        self.preprocess_data()\n        \n        # Engineer features\n        train_features = self.engineer_features('train')\n        test_features = self.engineer_features('test')\n        \n        # Prepare model data\n        train_features = self.prepare_model_data(train_features)\n        test_features = self.prepare_model_data(test_features)\n        \n        # Separate features and target\n        feature_cols = [col for col in train_features.columns if col not in ['msno', 'is_churn']]\n        self.feature_names = feature_cols\n        \n        X = train_features[feature_cols]\n        y = train_features['is_churn']\n        X_test = test_features[feature_cols]\n        \n        # Split training data\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, \n                                                          random_state=42, stratify=y)\n        \n        print(f\"Training set: {X_train.shape}\")\n        print(f\"Validation set: {X_val.shape}\")\n        print(f\"Test set: {X_test.shape}\")\n        print(f\"Churn rate in training: {y_train.mean():.3f}\")\n        \n        # Train models\n        results = self.train_models(X_train, y_train, X_val, y_val)\n        \n        # Create ensemble\n        ensemble_proba = self.create_ensemble(X_val, y_val)\n        \n        # Generate test predictions\n        test_predictions = self.generate_predictions(X_test)\n        \n        # Create submission file\n        submission = pd.DataFrame({\n            'msno': test_features['msno'],\n            'is_churn': test_predictions\n        })\n        submission.to_csv('submission.csv', index=False)\n        print(\"Submission file saved as 'submission.csv'\")\n        \n        return submission\n\n# Run the model\nif __name__ == \"__main__\":\n    model = ChurnPredictionModel()\n    \n    # Run complete pipeline\n    submission = model.run_complete_pipeline()\n    \n    # Plot feature importance\n    model.plot_feature_importance()\n    \n    print(\"Model training completed!\")\n    print(f\"Final submission shape: {submission.shape}\")\n    print(\"\\nSample predictions:\")\n    print(submission.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:51:58.565943Z","iopub.execute_input":"2025-08-15T16:51:58.566530Z","iopub.status.idle":"2025-08-15T16:54:30.130705Z","shell.execute_reply.started":"2025-08-15T16:51:58.566506Z","shell.execute_reply":"2025-08-15T16:54:30.129685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass ChurnPredictionModel:\n    def __init__(self):\n        self.models = {}\n        self.feature_importance = {}\n        self.scaler = StandardScaler()\n        self.label_encoders = {}\n        \n    def load_data(self):\n        \"\"\"Load all datasets\"\"\"\n        print(\"Loading datasets...\")\n        \n        # Load all datasets\n        self.train_data = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/train_data.csv')\n        self.members = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/members.csv')\n        self.transactions = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/transactions.csv')\n        self.user_logs = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/user_logs.csv')\n        self.test_data = pd.read_csv('/kaggle/input/customer-retention-datathon-apac-edition/kaggle_test_data.csv')\n        \n        print(f\"Train data shape: {self.train_data.shape}\")\n        print(f\"Members data shape: {self.members.shape}\")\n        print(f\"Transactions data shape: {self.transactions.shape}\")\n        print(f\"User logs data shape: {self.user_logs.shape}\")\n        print(f\"Test data shape: {self.test_data.shape}\")\n        \n        # Basic data validation\n        print(f\"\\nChurn rate in training data: {self.train_data['is_churn'].mean():.3f}\")\n        print(f\"Unique users in train: {self.train_data['msno'].nunique()}\")\n        print(f\"Unique users in test: {self.test_data['msno'].nunique()}\")\n        print(f\"Unique users in members: {self.members['msno'].nunique()}\")\n        print(f\"Unique users in transactions: {self.transactions['msno'].nunique()}\")\n        print(f\"Unique users in user_logs: {self.user_logs['msno'].nunique()}\")\n        \n    def preprocess_data(self):\n        \"\"\"Clean and preprocess all datasets\"\"\"\n        print(\"Preprocessing data...\")\n        \n        # Clean members data\n        self.members['bd'] = self.members['bd'].apply(lambda x: x if 0 <= x <= 100 else np.nan)\n        self.members['registration_init_time'] = pd.to_datetime(self.members['registration_init_time'], format='%Y%m%d')\n        \n        # Clean transactions data\n        self.transactions['transaction_date'] = pd.to_datetime(self.transactions['transaction_date'], format='%Y%m%d')\n        self.transactions['membership_expire_date'] = pd.to_datetime(self.transactions['membership_expire_date'], format='%Y%m%d')\n        \n        # Clean user logs data\n        self.user_logs['date'] = pd.to_datetime(self.user_logs['date'], format='%Y%m%d')\n        \n    def engineer_features(self, data_type='train'):\n        \"\"\"Engineer features from all data sources\"\"\"\n        print(f\"Engineering features for {data_type} data...\")\n        \n        if data_type == 'train':\n            base_data = self.train_data.copy()\n        else:\n            base_data = self.test_data.copy()\n            \n        # Start with base data\n        features_df = base_data.copy()\n        \n        # 1. Member features\n        member_features = self.create_member_features()\n        features_df = features_df.merge(member_features, on='msno', how='left')\n        \n        # 2. Transaction features\n        transaction_features = self.create_transaction_features()\n        features_df = features_df.merge(transaction_features, on='msno', how='left')\n        \n        # 3. User behavior features\n        behavior_features = self.create_behavior_features()\n        features_df = features_df.merge(behavior_features, on='msno', how='left')\n        \n        return features_df\n    \n    def create_member_features(self):\n        \"\"\"Create features from members data\"\"\"\n        member_features = self.members.copy()\n        \n        # Age features\n        member_features['age'] = member_features['bd']\n        member_features['age_group'] = pd.cut(member_features['age'], \n                                            bins=[0, 18, 25, 35, 45, 55, 100], \n                                            labels=['<18', '18-25', '25-35', '35-45', '45-55', '55+'])\n        \n        # Registration features\n        member_features['registration_year'] = member_features['registration_init_time'].dt.year\n        member_features['registration_month'] = member_features['registration_init_time'].dt.month\n        member_features['days_since_registration'] = (pd.Timestamp('2017-03-31') - member_features['registration_init_time']).dt.days\n        \n        return member_features[['msno', 'city', 'age', 'age_group', 'gender', 'registered_via', \n                               'registration_year', 'registration_month', 'days_since_registration']].copy()\n    \n    def create_transaction_features(self):\n        \"\"\"Create features from transaction data\"\"\"\n        # Get latest transaction for each user\n        latest_transactions = self.transactions.sort_values('transaction_date').groupby('msno').tail(1)\n        \n        # Aggregate transaction features\n        transaction_agg = self.transactions.groupby('msno').agg({\n            'payment_method_id': ['nunique', 'last'],\n            'payment_plan_days': ['mean', 'std', 'last'],\n            'plan_list_price': ['mean', 'std', 'last'],\n            'actual_amount_paid': ['mean', 'std', 'sum', 'last'],\n            'is_auto_renew': ['last', 'mean'],\n            'is_cancel': ['sum', 'mean', 'last'],\n            'transaction_date': ['count', 'last']\n        }).round(2)\n        \n        # Flatten column names\n        transaction_agg.columns = ['_'.join(col).strip() for col in transaction_agg.columns]\n        transaction_agg = transaction_agg.reset_index()\n        \n        # Add features from latest transaction\n        latest_features = latest_transactions[['msno', 'membership_expire_date']].copy()\n        latest_features['days_to_expire'] = (latest_features['membership_expire_date'] - pd.Timestamp('2017-03-31')).dt.days\n        \n        # Drop the datetime column, keep only the numeric feature\n        latest_features = latest_features[['msno', 'days_to_expire']]\n        \n        # Merge features\n        transaction_features = transaction_agg.merge(latest_features, on='msno', how='left')\n        \n        # Calculate discount rate\n        transaction_features['discount_rate'] = (\n            transaction_features['plan_list_price_last'] - transaction_features['actual_amount_paid_last']\n        ) / transaction_features['plan_list_price_last']\n        \n        return transaction_features\n    \n    def create_behavior_features(self):\n        \"\"\"Create features from user logs data\"\"\"\n        # Calculate behavior metrics for each user\n        behavior_agg = self.user_logs.groupby('msno').agg({\n            'num_25': ['sum', 'mean', 'std'],\n            'num_50': ['sum', 'mean', 'std'],\n            'num_75': ['sum', 'mean', 'std'],\n            'num_985': ['sum', 'mean', 'std'],\n            'num_100': ['sum', 'mean', 'std'],\n            'num_unq': ['sum', 'mean', 'std', 'max'],\n            'total_secs': ['sum', 'mean', 'std', 'max'],\n            'date': ['count']  # number of active days\n        }).round(2)\n        \n        # Flatten column names\n        behavior_agg.columns = ['_'.join(col).strip() for col in behavior_agg.columns]\n        behavior_agg = behavior_agg.reset_index()\n        \n        # Calculate derived features\n        total_songs = (behavior_agg['num_25_sum'] + behavior_agg['num_50_sum'] + \n                      behavior_agg['num_75_sum'] + behavior_agg['num_985_sum'] + \n                      behavior_agg['num_100_sum'])\n        \n        behavior_agg['total_songs'] = total_songs\n        behavior_agg['completion_rate'] = behavior_agg['num_100_sum'] / (total_songs + 1)\n        behavior_agg['skip_rate'] = behavior_agg['num_25_sum'] / (total_songs + 1)\n        behavior_agg['engagement_score'] = (behavior_agg['num_75_sum'] + behavior_agg['num_985_sum'] + \n                                           behavior_agg['num_100_sum']) / (total_songs + 1)\n        behavior_agg['avg_daily_songs'] = total_songs / (behavior_agg['date_count'] + 1)\n        behavior_agg['avg_daily_listening_time'] = behavior_agg['total_secs_sum'] / (behavior_agg['date_count'] + 1)\n        \n        return behavior_agg\n    \n    def prepare_model_data(self, features_df, is_train=True):\n        \"\"\"Prepare data for modeling\"\"\"\n        # Drop datetime columns that shouldn't be in the model\n        datetime_cols = features_df.select_dtypes(include=['datetime64']).columns\n        features_df = features_df.drop(columns=datetime_cols)\n        \n        # Encode categorical variables\n        categorical_cols = ['city', 'age_group', 'gender', 'registered_via']\n        \n        for col in categorical_cols:\n            if col in features_df.columns:\n                if is_train:\n                    # Fit and transform for training data\n                    if col not in self.label_encoders:\n                        self.label_encoders[col] = LabelEncoder()\n                        features_df[col] = self.label_encoders[col].fit_transform(features_df[col].astype(str))\n                    else:\n                        features_df[col] = self.label_encoders[col].fit_transform(features_df[col].astype(str))\n                else:\n                    # Transform only for test data, handle unseen categories\n                    if col in self.label_encoders:\n                        # Handle unseen categories by mapping them to a default value\n                        original_classes = set(self.label_encoders[col].classes_)\n                        features_df[col] = features_df[col].astype(str)\n                        \n                        # Map unseen categories to the most frequent class\n                        most_frequent_class = self.label_encoders[col].classes_[0]\n                        features_df[col] = features_df[col].apply(\n                            lambda x: x if x in original_classes else most_frequent_class\n                        )\n                        features_df[col] = self.label_encoders[col].transform(features_df[col])\n                    else:\n                        # If encoder doesn't exist, fill with 0\n                        features_df[col] = 0\n        \n        # Fill missing values\n        numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n        features_df[numeric_cols] = features_df[numeric_cols].fillna(features_df[numeric_cols].median())\n        \n        # Ensure all columns are numeric\n        for col in features_df.columns:\n            if col not in ['msno', 'is_churn']:\n                features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n        \n        # Fill any remaining NaN values created by conversion\n        numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n        features_df[numeric_cols] = features_df[numeric_cols].fillna(0)\n        \n        return features_df\n    \n    def train_models(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train multiple models\"\"\"\n        print(\"Training models...\")\n        \n        # Define models\n        models = {\n            'random_forest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n            'gradient_boosting': GradientBoostingClassifier(n_estimators=200, random_state=42),\n            'logistic_regression': LogisticRegression(random_state=42),\n            'lightgbm': lgb.LGBMClassifier(random_state=42, n_jobs=-1),\n            'xgboost': xgb.XGBClassifier(random_state=42, n_jobs=-1)\n        }\n        \n        results = {}\n        \n        for name, model in models.items():\n            print(f\"Training {name}...\")\n            \n            try:\n                # Train model\n                model.fit(X_train, y_train)\n                \n                # Predict on validation set\n                y_pred = model.predict(X_val)\n                y_pred_proba = model.predict_proba(X_val)[:, 1]\n                \n                # Calculate metrics\n                auc_score = roc_auc_score(y_val, y_pred_proba)\n                \n                results[name] = {\n                    'model': model,\n                    'auc_score': auc_score,\n                    'predictions': y_pred,\n                    'probabilities': y_pred_proba\n                }\n                \n                print(f\"{name} AUC: {auc_score:.4f}\")\n                \n                # Store feature importance for tree-based models\n                if hasattr(model, 'feature_importances_'):\n                    self.feature_importance[name] = model.feature_importances_\n                    \n            except Exception as e:\n                print(f\"Error training {name}: {str(e)}\")\n                continue\n        \n        self.models = results\n        return results\n    \n    def create_ensemble(self, X_val, y_val):\n        \"\"\"Create ensemble prediction\"\"\"\n        print(\"Creating ensemble...\")\n        \n        if not self.models:\n            raise ValueError(\"No models were successfully trained!\")\n        \n        # Weight models by their AUC scores\n        total_auc = sum([result['auc_score'] for result in self.models.values()])\n        weights = {name: result['auc_score'] / total_auc for name, result in self.models.items()}\n        \n        # Create weighted ensemble\n        ensemble_proba = np.zeros(len(X_val))\n        for name, result in self.models.items():\n            ensemble_proba += weights[name] * result['probabilities']\n        \n        ensemble_auc = roc_auc_score(y_val, ensemble_proba)\n        print(f\"Ensemble AUC: {ensemble_auc:.4f}\")\n        \n        self.ensemble_weights = weights\n        return ensemble_proba\n    \n    def plot_feature_importance(self, model_name='lightgbm', top_n=20):\n        \"\"\"Plot feature importance\"\"\"\n        if model_name in self.feature_importance:\n            feature_names = self.feature_names\n            importance = self.feature_importance[model_name]\n            \n            # Get top features\n            feature_importance_df = pd.DataFrame({\n                'feature': feature_names,\n                'importance': importance\n            }).sort_values('importance', ascending=False).head(top_n)\n            \n            plt.figure(figsize=(10, 8))\n            sns.barplot(data=feature_importance_df, x='importance', y='feature')\n            plt.title(f'Top {top_n} Feature Importance - {model_name.upper()}')\n            plt.tight_layout()\n            plt.show()\n    \n    def generate_predictions(self, test_features):\n        \"\"\"Generate final predictions for test set\"\"\"\n        print(\"Generating final predictions...\")\n        \n        # Use ensemble approach\n        ensemble_proba = np.zeros(len(test_features))\n        \n        for name, weight in self.ensemble_weights.items():\n            model = self.models[name]['model']\n            proba = model.predict_proba(test_features)[:, 1]\n            ensemble_proba += weight * proba\n        \n        return ensemble_proba\n    \n    def run_complete_pipeline(self):\n        \"\"\"Run the complete modeling pipeline\"\"\"\n        print(\"=== CHURN PREDICTION MODEL PIPELINE ===\")\n        \n        # Load and preprocess data\n        self.load_data()\n        self.preprocess_data()\n        \n        # Engineer features\n        train_features = self.engineer_features('train')\n        test_features = self.engineer_features('test')\n        \n        # Prepare model data\n        train_features = self.prepare_model_data(train_features, is_train=True)\n        test_features = self.prepare_model_data(test_features, is_train=False)\n        \n        # Separate features and target\n        feature_cols = [col for col in train_features.columns if col not in ['msno', 'is_churn']]\n        self.feature_names = feature_cols\n        \n        print(f\"Feature columns: {len(feature_cols)}\")\n        print(f\"Data types in train_features:\")\n        print(train_features[feature_cols].dtypes.value_counts())\n        \n        X = train_features[feature_cols].copy()\n        y = train_features['is_churn'].copy()\n        X_test = test_features[feature_cols].copy()\n        \n        # Ensure all feature columns are numeric\n        print(\"Converting all features to numeric...\")\n        for col in feature_cols:\n            X[col] = pd.to_numeric(X[col], errors='coerce')\n            X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n        \n        # Fill any NaN values\n        X = X.fillna(0)\n        X_test = X_test.fillna(0)\n        \n        print(f\"Final data types:\")\n        print(X.dtypes.value_counts())\n        \n        # Final validation - ensure no object or datetime columns\n        non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n        if len(non_numeric_cols) > 0:\n            print(f\"Warning: Found non-numeric columns: {list(non_numeric_cols)}\")\n            for col in non_numeric_cols:\n                X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)\n                X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n        \n        print(f\"Data validation complete. All columns are numeric: {X.dtypes.apply(lambda x: np.issubdtype(x, np.number)).all()}\")\n        \n        # Split training data\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, \n                                                          random_state=42, stratify=y)\n        \n        print(f\"Training set: {X_train.shape}\")\n        print(f\"Validation set: {X_val.shape}\")\n        print(f\"Test set: {X_test.shape}\")\n        print(f\"Churn rate in training: {y_train.mean():.3f}\")\n        \n        # Train models\n        results = self.train_models(X_train, y_train, X_val, y_val)\n        \n        # Create ensemble\n        ensemble_proba = self.create_ensemble(X_val, y_val)\n        \n        # Generate test predictions\n        test_predictions = self.generate_predictions(X_test)\n        \n        # Create submission file\n        submission = pd.DataFrame({\n            'msno': test_features['msno'],\n            'is_churn': test_predictions\n        })\n        submission.to_csv('submission.csv', index=False)\n        print(\"Submission file saved as 'submission.csv'\")\n        \n        return submission\n\n# Run the model\nif __name__ == \"__main__\":\n    model = ChurnPredictionModel()\n    \n    # Run complete pipeline\n    submission = model.run_complete_pipeline()\n    \n    # Plot feature importance\n    model.plot_feature_importance()\n    \n    print(\"Model training completed!\")\n    print(f\"Final submission shape: {submission.shape}\")\n    print(\"\\nSample predictions:\")\n    print(submission.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:59:05.161486Z","iopub.execute_input":"2025-08-15T16:59:05.162152Z","iopub.status.idle":"2025-08-15T17:01:57.757315Z","shell.execute_reply.started":"2025-08-15T16:59:05.162130Z","shell.execute_reply":"2025-08-15T17:01:57.756485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}